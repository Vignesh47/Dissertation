{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B74OJi2U3qTN"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plan of Action\n",
        "\n",
        "1. Load Play Store App Reviews dataset (12,000 reviews)\n",
        "2. Pre-process dataset by removing special characters, numbers, etc. from user reviews + convert sentiment labels positive & negative to numbers 1 & 0, respectively\n",
        "3. Import GloVe Word Embedding to build Embedding Dictionary + Use this to build Embedding Matrix for our Corpus\n",
        "4. Model Training using Deep Learning in Keras for separate: Simple Neural Net, CNN and LSTM Models and analyse model performance and results\n",
        "5. Last, perform predictions on real App Store reviews"
      ],
      "metadata": {
        "id": "KT9DWuBmDYiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "97l72sLYDagj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-v9ehzTYu0H"
      },
      "source": [
        "**Installing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAmG79c3kM5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1317599-8816-4347-e7ef-f5cc1dcb7926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google_trans_new in /usr/local/lib/python3.7/dist-packages (1.1.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.10.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install google_trans_new\n",
        "!pip install textblob\n",
        "!pip install googletrans==3.1.0a0\n",
        "!pip install transformers\n",
        "!pip install contractions\n",
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95eM3CsNY09s"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqA9KB05kmiz"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "from  nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk.corpus\n",
        "import string\n",
        "import googletrans\n",
        "from nltk.corpus import words\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "#Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "#model library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate\n",
        "from collections import Counter\n",
        "import lightgbm as lgb  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFS5MouDZH9j"
      },
      "source": [
        " **Creating Objects**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrRKctx8lmN1",
        "outputId": "de8a9f37-66c8-40dc-9800-04ffb213cc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "#nlp = English()\n",
        "strin = string.punctuation\n",
        "translator = googletrans.Translator()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.remove('not')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYLFs_UXZPjY"
      },
      "source": [
        "**Loading File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjXu4lN1lsIk"
      },
      "outputs": [],
      "source": [
        "#rest will convert the file into dataframes\n",
        "rest = pd.read_csv('/content/sample_data/reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3js0oVMCiOy",
        "outputId": "33ea4035-d845-4c45-bce7-8183e1ed83d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12495, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#validating the first 5 record in the dataframes\n",
        "rest.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rest['pred'] = rest.score.apply(lambda x: 0 if x in [1, 2,3] else 1)"
      ],
      "metadata": {
        "id": "LweWXaKA9YfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL IMPLEMENTATION"
      ],
      "metadata": {
        "id": "oqL6jeDd9Dtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "X_train, X_test, y_train, y_test = train_test_split(rest.content,rest.pred, test_size=0.2, random_state=1,stratify= rest.pred)"
      ],
      "metadata": {
        "id": "MGjftx2k9DGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c8iV4BcZjIz"
      },
      "source": [
        "**Data Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqBaCukNmKVM"
      },
      "outputs": [],
      "source": [
        "def clean_text(Reviews):\n",
        "  new_reviews = translator.translate(Reviews,dest='en', src='auto').text\n",
        "  #src.append(translator.translate(Reviews).src)\n",
        "  Reviews = ''.join([contractions.fix(new_reviews)])\n",
        "  text = Reviews.lower()\n",
        "  punct = ''.join([ch for ch in text if ch not in string.punctuation])\n",
        "  punct = re.sub(r'[^\\w\\s]+',\"\",punct)\n",
        "  remove_number = re.sub('\\d+','',punct)\n",
        "  repeat_text = re.sub(r'(.)\\1+', r'\\1\\1', remove_number)\n",
        "  text = repeat_text.strip()\n",
        "  text = ' '.join(text.split())\n",
        "  text = word_tokenize(text)\n",
        "  text= [word.lower() for word in text if word.lower() not in stop_words]\n",
        "  lemmatized_string = ' '.join([lemmatizer.lemmatize(words) for words in text])\n",
        "  return lemmatized_string "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEIhw1k570xF"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.apply(lambda x : clean_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjPZLhibdrqE"
      },
      "source": [
        "**Model Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X2vHvk3_PR7"
      },
      "outputs": [],
      "source": [
        "# Creating Model\n",
        "tfidf = TfidfVectorizer(analyzer='word',ngram_range=(1,3),)\n",
        "tfidf.fit(list(X_train))\n",
        "tf_idf_vector_train = tfidf.transform(X_train)\n",
        "tf_idf_vector_test = tfidf.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=2)\n",
        "X_train_smt, y_train_smt = sm.fit_resample(tf_idf_vector_train, y_train.ravel())\n",
        "print(X_train_smt.shape,y_train_smt.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEUYOTXJ9vjr",
        "outputId": "8eb004fd-0bfe-47f0-d2a7-dc54b313f5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10946, 206558) (10946,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word EMBEDDING**"
      ],
      "metadata": {
        "id": "DIh8QTj835Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(lower=False)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "train_text_vec = tokenizer.texts_to_sequences(X_train)\n",
        "tokenizer.fit_on_texts(X_test)\n",
        "test_text_vec = tokenizer.texts_to_sequences(X_test)\n",
        "test_text_vec\n",
        "'''"
      ],
      "metadata": {
        "id": "6WUnegy734r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uCTTHED_gFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b39f6ff-e325-48ef-9e3e-3f2917a2ef89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Train R2 Score: 0.808\n",
            "RF Test R2 Score: 0.725\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(max_depth=60, min_samples_leaf=5,\n",
        "                            min_samples_split=6, n_estimators=60, oob_score=True,\n",
        "                            max_leaf_nodes=55, random_state=13)\n",
        "rf.fit(X_train_smt, y_train_smt)\n",
        "predictions_rf_test = pd.Series(rf.predict(tf_idf_vector_test))\n",
        "\n",
        "train_scoreRF = rf.score(X_train_smt, y_train_smt)\n",
        "test_scoreRF = rf.score(tf_idf_vector_test, y_test)\n",
        "\n",
        "# Train and Test R2 Scores\n",
        "print(f\"RF Train R2 Score: {round(train_scoreRF, 3)}\")\n",
        "print(f\"RF Test R2 Score: {round(test_scoreRF, 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pGNbmfX_7p0",
        "outputId": "6e320646-894c-43e4-db43-f0bb7521b320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Train R2 Score: 0.93\n",
            "LR Test R2 Score: 0.812\n"
          ]
        }
      ],
      "source": [
        "lr = LogisticRegression(random_state=13, max_iter=2000).fit(X_train_smt, y_train_smt)\n",
        "\n",
        "predictions_LR_test = pd.Series(lr.predict(tf_idf_vector_test))\n",
        "\n",
        "train_scoreLR = lr.score(X_train_smt, y_train_smt)\n",
        "test_scoreLR = lr.score(tf_idf_vector_test, y_test)\n",
        "\n",
        "# Train and Test R2 Scores\n",
        "print(f\"LR Train R2 Score: {round(train_scoreLR, 3)}\")\n",
        "print(f\"LR Test R2 Score: {round(test_scoreLR, 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnNAKM7s1IBA",
        "outputId": "a29dee54-d824-4351-bdaf-e6115ff61290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Train R2 Score: 0.889\n",
            "LGB Test R2 Score: 0.783\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb \n",
        "lgb = lgb.LGBMClassifier().fit(tf_idf_vector_train, y_train)\n",
        "\n",
        "predictions_LGB_test = pd.Series(lgb.predict(tf_idf_vector_test))\n",
        "\n",
        "train_scoreLGB = lgb.score(tf_idf_vector_train, y_train)\n",
        "test_scoreLGB = lgb.score(tf_idf_vector_test, y_test)\n",
        "\n",
        "# Train and Test R2 Scores\n",
        "print(f\"LGB Train R2 Score: {round(train_scoreLGB, 3)}\")\n",
        "print(f\"LGB Test R2 Score: {round(test_scoreLGB, 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "CNB = ComplementNB()\n",
        "CNB.fit(tf_idf_vector_train, y_train)\n",
        "\n",
        "\n",
        "train_scoreCNB = CNB.score(tf_idf_vector_train, y_train)\n",
        "test_scoreCNB = CNB.score(tf_idf_vector_test, y_test)\n",
        "\n",
        "predicted = CNB.predict(tf_idf_vector_test)\n",
        "\n",
        "# Train and Test R2 Scores\n",
        "print(f\"LGB Train R2 Score: {round(train_scoreCNB, 3)}\")\n",
        "print(f\"LGB Test R2 Score: {round(test_scoreCNB, 3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfw8fV4Cyc3k",
        "outputId": "d862aa29-8c60-42db-cbfc-5567961c4f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Train R2 Score: 0.959\n",
            "LGB Test R2 Score: 0.814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MNB = MultinomialNB()\n",
        "MNB.fit(tf_idf_vector_train, y_train)\n",
        "predictions_MNB_test = pd.Series(MNB.predict(tf_idf_vector_test))\n",
        "\n",
        "train_scoreMNB = MNB.score(tf_idf_vector_train, y_train)\n",
        "test_scoreMNB = MNB.score(tf_idf_vector_test, y_test)\n",
        "\n",
        "# Train and Test R2 Scores\n",
        "print(f\"LGB Train R2 Score: {round(train_scoreMNB, 3)}\")\n",
        "print(f\"LGB Test R2 Score: {round(test_scoreMNB, 3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3jhHAGrzlUz",
        "outputId": "288be711-dff5-4215-855d-d39233c2f197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGB Train R2 Score: 0.948\n",
            "LGB Test R2 Score: 0.792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data= \"nothing missing\"\n",
        "clean_data = clean_text(test_data)\n",
        "print(clean_data)\n",
        "tf_idf_vector_test = tfidf.transform([clean_data])\n",
        "res=lr.predict(tf_idf_vector_test)\n",
        "res = np.array(res[0])\n",
        "if res == 1:\n",
        "  print('Sentiment type:Positive',res)\n",
        "elif res == 0:\n",
        "  print('Sentiment type:Negative',res)\n",
        "  print('==================================================\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mHbR9ClGci8",
        "outputId": "689e7e9d-1b2e-46b4-f6bc-22329a19612c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nothing missing\n",
            "Sentiment type:Negative 0\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "E9ueG7Bs4zMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trans(new):\n",
        "\n",
        "  gfg = TextBlob(new).correct\n",
        " \n",
        "  # Reviews = ''.join([contractions.fix(new)])\n",
        "  # new_reviews = translator.translate(Reviews,dest='en', src='auto').text\n",
        "  # #src.append(translator.translate(Reviews).src)\n",
        "  \n",
        "  return gfg"
      ],
      "metadata": {
        "id": "fIrk2ZL44DyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing = \"y cudn't enuf alone \""
      ],
      "metadata": {
        "id": "cLjlqnS94LcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_new_data_file = trans(testing)\n",
        "clean_new_data_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6558RXB4YGj",
        "outputId": "8a2b353d-f7b9-48da-e25b-9980951450a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method BaseBlob.correct of TextBlob(\"y cudn't enuf alone \")>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}